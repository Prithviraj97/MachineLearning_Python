{"cells":[{"cell_type":"markdown","id":"7607bdb2-1707-4361-a984-1c443fe84370","metadata":{},"source":["_add text here_"]},{"cell_type":"code","execution_count":1,"id":"7f26dd6a-20aa-4d19-8d3f-b8f64fba1821","metadata":{"executionCancelledAt":null,"executionTime":32,"lastExecutedAt":1703539138018,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Write and run code here\nfrom sklearn import datasets\nfrom sklearn.preprocessing import train_test_split\nfrom sklearn.model_selection import LogisticRegression\ndigits = datasets.load_digits()\nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n# Apply logistic regression and print scores\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(lr.score(X_test, y_test))\nprint(lr.score(X_test,y_test))\n\nfrom sklearn.svm import SVC,LinearSVC\n# Apply SVM and print scores\nsvm = LinearSVC()\nsvm.fit(X_train, y_train)\nprint(svm.score(X_test,y_test))\nprint(svm.score(X_test,y_test))"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'LogisticRegression' from 'sklearn.model_selection' (c:\\Users\\TheEarthG\\.conda\\envs\\Forage\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      5\u001b[0m digits \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_digits()\n\u001b[0;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(digits\u001b[38;5;241m.\u001b[39mdata, digits\u001b[38;5;241m.\u001b[39mtarget)\n","\u001b[1;31mImportError\u001b[0m: cannot import name 'LogisticRegression' from 'sklearn.model_selection' (c:\\Users\\TheEarthG\\.conda\\envs\\Forage\\Lib\\site-packages\\sklearn\\model_selection\\__init__.py)"]}],"source":["# Write and run code here\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import LogisticRegression\n","digits = datasets.load_digits()\n","X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n","\n","# Apply logistic regression and print scores\n","lr = LogisticRegression()\n","lr.fit(X_train, y_train)\n","print(lr.score(X_test, y_test))\n","print(lr.score(X_test,y_test))\n","\n","from sklearn.svm import SVC,LinearSVC\n","# Apply SVM and print scores\n","svm = LinearSVC()\n","svm.fit(X_train, y_train)\n","print(svm.score(X_test,y_test))\n","print(svm.score(X_test,y_test))"]},{"cell_type":"code","execution_count":null,"id":"050f1adb-2fdd-42d4-bc79-202d5193aba4","metadata":{},"outputs":[],"source":["# Instantiate logistic regression and train\n","lr = LogisticRegression()\n","lr.fit(X,y)\n","\n","# Predict sentiment for a glowing review\n","review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n","review1_features = get_features(review1)\n","print(\"Review:\", review1)\n","print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n","\n","# Predict sentiment for a poor review\n","review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n","review2_features = get_features(review2)\n","print(\"Review:\", review2)\n","print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"]},{"cell_type":"code","execution_count":null,"id":"3b0bc718-b917-41bc-841e-f421b072987a","metadata":{},"outputs":[],"source":["'''A subset of scikit-learn's built-in wine dataset is already loaded into X, along with binary labels in y.'''\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# Define the classifiers\n","classifiers = [LogisticRegression(), LinearSVC(), SVC(), KNeighborsClassifier()]\n","\n","# Fit the classifiers\n","for c in classifiers:\n","    c.fit(X,y)\n","\n","# Plot the classifiers\n","plot_4_classifiers(X, y, classifiers)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"09b1d918-9e67-4b83-b858-2dbf695f3825","metadata":{},"outputs":[],"source":["# Set the coefficients\n","model.coef_ = np.array([[-1,1]])\n","model.intercept_ = np.array([-3])\n","\n","# Plot the data and decision boundary\n","plot_classifier(X,y,model)\n","\n","# Print the number of errors\n","num_err = np.sum(y != model.predict(X))\n","print(\"Number of errors:\", num_err)"]},{"cell_type":"code","execution_count":null,"id":"543f2729-cb85-46f2-8b7a-4ffa6986e211","metadata":{},"outputs":[],"source":["# The squared error, summed over training examples\n","def my_loss(w):\n","    s = 0\n","    for i in range(y.size):\n","        # Get the true and predicted target values for example 'i'\n","        y_i_true = y[i]\n","        y_i_pred = w@X[i]\n","        s = s + (y_i_true-y_i_pred)**2\n","    return s\n","\n","# Returns the w that makes my_loss(w) smallest\n","w_fit = minimize(my_loss, X[0]).x\n","print(w_fit)\n","\n","# Compare with scikit-learn's LinearRegression coefficients\n","lr = LinearRegression(fit_intercept=False).fit(X,y)\n","print(lr.coef_)"]},{"cell_type":"code","execution_count":null,"id":"5b359e9f-3167-4751-a02e-96d7da6aa27b","metadata":{},"outputs":[],"source":["# Mathematical functions for logistic and hinge losses\n","def log_loss(raw_model_output):\n","   return np.log(1+np.exp(-raw_model_output))\n","def hinge_loss(raw_model_output):\n","   return np.maximum(0,1-raw_model_output)\n","\n","# Create a grid of values and plot\n","grid = np.linspace(-2,2,1000)\n","plt.plot(grid, log_loss(grid), label='logistic')\n","plt.plot(grid, hinge_loss(grid), label='hinge')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"6003b2be-bba7-4010-b804-cf05886c34f4","metadata":{},"outputs":[],"source":["# The logistic loss, summed over training examples\n","def my_loss(w):\n","    s = 0\n","    for i in range(0, 569):\n","        raw_model_output = w@X[i]\n","        s = s + log_loss(raw_model_output * y[i])\n","    return s\n","\n","# Returns the w that makes my_loss(w) smallest\n","w_fit = minimize(my_loss, X[0]).x\n","print(w_fit)\n","\n","# Compare with scikit-learn's LogisticRegression\n","lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)\n","print(lr.coef_)"]},{"cell_type":"code","execution_count":null,"id":"925bc4b4-34ea-4334-9a9d-f3f209a7d26c","metadata":{},"outputs":[],"source":["# Train and validaton errors initialized as empty list\n","train_errs = list()\n","valid_errs = list()\n","\n","# Loop over values of C_value\n","for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n","    # Create LogisticRegression object and fit\n","    lr = LogisticRegression(C=C_value)\n","    lr.fit(X_train, y_train)\n","    \n","    # Evaluate error rates and append to lists\n","    train_errs.append( 1.0 - lr.score(X_train, y_train) )\n","    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )\n","    \n","# Plot results\n","plt.semilogx(C_values, train_errs, C_values, valid_errs)\n","plt.legend((\"train\", \"validation\"))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"3d7c98ff-9af2-4c6e-8496-cc85f2951a3f","metadata":{},"outputs":[],"source":["# Specify L1 regularization\n","lr = LogisticRegression(solver='liblinear', penalty='l1')\n","\n","# Instantiate the GridSearchCV object and run the search\n","searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})\n","searcher.fit(X_train, y_train)\n","\n","# Report the best parameters\n","print(\"Best CV params\", searcher.best_params_)\n","\n","# Find the number of nonzero coefficients (selected features)\n","best_lr = searcher.best_estimator_\n","coefs = best_lr.coef_\n","print(\"Total number of features:\", coefs.size)\n","print(\"Number of selected features:\", np.count_nonzero(coefs))"]},{"cell_type":"code","execution_count":null,"id":"15297006-0534-416a-aa66-35c74bab6136","metadata":{},"outputs":[],"source":["# Get the indices of the sorted cofficients\n","inds_ascending = np.argsort(lr.coef_.flatten()) \n","inds_descending = inds_ascending[::-1]\n","\n","# Print the most positive words\n","print(\"Most positive words: \", end=\"\")\n","for i in range(5):\n","    print(vocab[inds_descending[i]], end=\", \")\n","print(\"\\n\")\n","\n","# Print most negative words\n","print(\"Most negative words: \", end=\"\")\n","for i in range(5):\n","    print(vocab[inds_ascending[i]], end=\", \")\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"id":"9cc8f648-9d7b-4ade-a639-8d572848db5d","metadata":{},"outputs":[],"source":["# Set the regularization strength\n","model = LogisticRegression(C=1)\n","\n","# Fit and plot\n","model.fit(X,y)\n","plot_classifier(X,y,model,proba=True)\n","\n","# Predict probabilities on training points\n","prob = model.predict_proba(X)\n","print(\"Maximum predicted probability\", np.max(prob))"]},{"cell_type":"code","execution_count":null,"id":"551996b3-0e4c-48df-ab9a-66aae259c143","metadata":{},"outputs":[],"source":["# Set the regularization strength\n","model = LogisticRegression(C=1)\n","\n","# Fit and plot\n","model.fit(X,y)\n","plot_classifier(X,y,model,proba=True)\n","\n","# Predict probabilities on training points\n","prob = model.predict_proba(X)\n","print(\"Maximum predicted probability\", prob)"]},{"cell_type":"code","execution_count":null,"id":"617abb69-1be6-464b-9147-f62cdef61731","metadata":{},"outputs":[],"source":["lr = LogisticRegression()\n","lr.fit(X,y)\n","\n","# Get predicted probabilities\n","proba = lr.predict_proba(X)\n","\n","# Sort the example indices by their maximum probability\n","proba_inds = np.argsort(np.max(proba,axis=1))\n","\n","# Show the most confident (least ambiguous) digit\n","show_digit(proba_inds[-1], lr)\n","\n","# Show the least confident (most ambiguous) digit\n","show_digit(proba_inds[0], lr)"]},{"cell_type":"code","execution_count":null,"id":"bd828911-f8f3-462f-ad77-406987b41094","metadata":{},"outputs":[],"source":["# Fit one-vs-rest logistic regression classifier\n","lr_ovr = LogisticRegression(multi_class='ovr')\n","lr_ovr.fit(X_train, y_train)\n","\n","print(\"OVR training accuracy:\", lr_ovr.score(X_train, y_train))\n","print(\"OVR test accuracy    :\", lr_ovr.score(X_test, y_test))\n","\n","# Fit softmax classifier\n","lr_mn = LogisticRegression(multi_class='multinomial')\n","lr_mn.fit(X_train, y_train)\n","\n","print(\"Softmax training accuracy:\", lr_mn.score(X_train, y_train))\n","print(\"Softmax test accuracy    :\", lr_mn.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"1f5b3972-0b9a-4447-a774-906b32e7f04b","metadata":{},"outputs":[],"source":["import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","import sys\n","sys.path.append('/usr/local/share/datasets/')\n","from plot_classifier import *\n","\n","import urllib.request\n","urllib.request.urlretrieve(\"https://assets.datacamp.com/production/repositories/2031/datasets/594cc94c67d72b835d504b6320dcba58e1fc77ca/multiData.txt\", \"multiData.npz\")\n","with open(\"multiData.npz\", \"rb\") as f:\n","    loaded_data = np.load(f)\n","    X_train = loaded_data['X']\n","    y_train = loaded_data['y']\n","    X_test = loaded_data['Xvalid']\n","    y_test = loaded_data['yvalid']\n","\n","lr_mn = LogisticRegression(multi_class=\"multinomial\", C=100)\n","lr_mn.fit(X_train, y_train)\n","\n","fig = plt.figure()\n","plt.title(\"lr_mn (softmax)\")\n","plot_classifier(X_train, y_train, lr_mn, ax=fig.gca())\n","plt.show()\n","\n","lr_ovr = LogisticRegression(multi_class=\"ovr\", C=100)\n","lr_ovr.fit(X_train, y_train)\n","\n","fig = plt.figure()\n","plt.title(\"lr_ovr (one-vs-rest)\")\n","plot_classifier(X_train, y_train, lr_ovr, ax=fig.gca())\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"7e79be76-ad1e-45e5-b024-6772932a81e6","metadata":{},"outputs":[],"source":["# Print training accuracies\n","print(\"Softmax     training accuracy:\", lr_mn.score(X_train, y_train))\n","print(\"One-vs-rest training accuracy:\", lr_ovr.score(X_train, y_train))\n","\n","# Create the binary classifier (class 1 vs. rest)\n","lr_class_1 = LogisticRegression(C=100)\n","lr_class_1.fit(X_train, y_train==1)\n","\n","# Plot the binary classifier (class 1 vs. rest)\n","plot_classifier(X_train, y_train==1, lr_class_1)"]},{"cell_type":"code","execution_count":null,"id":"c7e69076-10f0-4544-8683-bd316cab7055","metadata":{},"outputs":[],"source":["# We'll use SVC instead of LinearSVC from now on\n","from sklearn.svm import SVC\n","\n","# Create/plot the binary classifier (class 1 vs. rest)\n","svm_class_1 = SVC()\n","svm_class_1.fit(X_train, y_train==1)\n","plot_classifier(X_train, y_train==1, svm_class_1)\n","\n","# Train a linear SVM\n","svm = SVC(kernel=\"linear\")\n","svm.fit(X,y)\n","plot_classifier(X, y, svm, lims=(11,15,0,6))\n","\n","# Make a new data set keeping only the support vectors\n","print(\"Number of original examples\", len(X))\n","print(\"Number of support vectors\", len(svm.support_))\n","X_small = X[svm.support_]\n","y_small = y[svm.support_]\n","\n","# Train a new SVM using only the support vectors\n","svm_small = SVC(kernel=\"linear\")\n","svm_small.fit(X_small, y_small)\n","plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))"]},{"cell_type":"code","execution_count":null,"id":"aa258244-c2a1-41b8-a05e-ff06031efa97","metadata":{},"outputs":[],"source":["from sklearn import datasets\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","\n","data = datasets.load_digits()\n","X = data.data\n","y = data.target == 2\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n","\n","X, y = X_train, y_train\n","# Instantiate an RBF SVM\n","svm = SVC()\n","\n","# Instantiate the GridSearchCV object and run the search\n","parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n","searcher = GridSearchCV(svm, parameters)\n","searcher.fit(X, y)\n","\n","# Report the best parameters\n","print(\"Best CV params\", searcher.best_params_)"]},{"cell_type":"code","execution_count":null,"id":"4e2698e1-71da-4321-8aee-2bcc522ca5a8","metadata":{},"outputs":[],"source":["# Instantiate an RBF SVM\n","svm = SVC()\n","\n","# Instantiate the GridSearchCV object and run the search\n","parameters = {'C':[0.1, 1, 10], 'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}\n","searcher = GridSearchCV(svm, parameters)\n","searcher.fit(X_train, y_train)\n","\n","# Report the best parameters and the corresponding score\n","print(\"Best CV params\", searcher.best_params_)\n","print(\"Best CV accuracy\", searcher.best_score_)\n","\n","# Report the test accuracy using these best parameters\n","print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"7cdfc54e-598b-4702-8ff2-acc0657322f7","metadata":{},"outputs":[],"source":["# We set random_state=0 for reproducibility \n","linear_classifier = SGDClassifier(random_state=0)\n","\n","# Instantiate the GridSearchCV object and run the search\n","parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], \n","             'loss':['log_loss', 'hinge']}\n","searcher = GridSearchCV(linear_classifier, parameters, cv=10)\n","searcher.fit(X_train, y_train)\n","\n","# Report the best parameters and the corresponding score\n","print(\"Best CV params\", searcher.best_params_)\n","print(\"Best CV accuracy\", searcher.best_score_)\n","print(\"Test accuracy of best grid search hypers:\", searcher.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"id":"9a00c4fe-68b9-4fdb-8be2-1a6007431827","metadata":{},"outputs":[],"source":[]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
