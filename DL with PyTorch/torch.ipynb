{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "df = pd.read_csv('https://assets.datacamp.com/production/repositories/6193/datasets/fca5067912db2b2346f568ce806915450fe56b99/water_potability.csv').to_numpy()\n",
    "zero = df[100, :]\n",
    "\n",
    "sample = torch.tensor([zero[:9]]).float()\n",
    "target = torch.tensor([[1, 0]]).float()\n",
    "\n",
    "weight = torch.randn(2, 9, requires_grad=True) \n",
    "bias = torch.randn(2, requires_grad=True)\n",
    "\n",
    "preds = sample.matmul(weight.t()) + bias\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat-y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)    \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total\n",
    "\n",
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 12), nn.Linear(12,8), nn.Linear(8,4),\n",
    "nn.Linear(4, n_classes))\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Manipulating the capacity of the network'''\n",
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 8),\n",
    "nn.Linear(8, 4),\n",
    "nn.Linear(4 ,n_classes))\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(np.array(\n",
    "  dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']])).float()\n",
    "target = torch.tensor(np.array(\n",
    "  dataframe['Potability'])).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(nn.Linear(4,2),nn.Linear(2,1))\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss/len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch \n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''implementing random search'''\n",
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor between 0.01 and 0.0001\n",
    "    factor = np.random.uniform(2,4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly select a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import convolve\n",
    "from PIL import Image\n",
    "\n",
    "class SIFTAlgorithm:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def gaussian_blur(self, img, sigma):\n",
    "        size = int(2*(np.ceil(3*sigma))+1)\n",
    "        x, y = np.mgrid[-size//2 + 1:size//2 + 1, -size//2 + 1:size//2 + 1]\n",
    "        g = np.exp(-(x**2 + y**2) / (2*sigma**2))\n",
    "        g /= g.sum()\n",
    "        img_blur = np.zeros_like(img)\n",
    "        for i in range(3):  \n",
    "            img_blur[:,:,  i] = convolve(img[:,:, i], g, mode='constant', cval=0)\n",
    "        return img_blur\n",
    "\n",
    "    def difference_of_gaussian(self, img1, img2):\n",
    "        return img1 - img2\n",
    "\n",
    "    def find_keypoints(self, dog_img, threshold=0.5):\n",
    "        keypoints = np.argwhere(dog_img > threshold)\n",
    "        return keypoints  # Return array of (row, col) positions\n",
    "\n",
    "    def generate_descriptors(self, keypoints, img):\n",
    "        descriptors = []\n",
    "        for kp in keypoints:\n",
    "            x, y = kp\n",
    "            if x <= 0 or x >= img.shape[0]-1 or y <= 0 or y >= img.shape[1]-1:\n",
    "                continue\n",
    "            # Simple descriptor: use surrounding pixel values\n",
    "            patch = img[x-1:x+2, y-1:y+2, :].flatten()\n",
    "            descriptors.append(patch)\n",
    "        return np.array(descriptors)\n",
    "\n",
    "    def out(self, img):\n",
    "        print(img.shape)\n",
    "        h, w, c = img.shape\n",
    "        img_gray = np.mean(img, axis=2)  \n",
    "\n",
    "        img_blur1 = self.gaussian_blur(img_gray, sigma=1)\n",
    "        img_blur2 = self.gaussian_blur(img_gray, sigma=2)\n",
    "        dog_img = self.difference_of_gaussian(img_blur1, img_blur2)\n",
    "\n",
    "        keypoints = self.find_keypoints(dog_img)\n",
    "\n",
    "        descriptors = self.generate_descriptors(keypoints, img_gray)\n",
    "\n",
    "        return keypoints, descriptors\n",
    "\n",
    "sift = SIFTAlgorithm()\n",
    "image = Image.open('11102.jpg')\n",
    "img = np.asarray(image)\n",
    "print(sift.out(img=img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Forage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
