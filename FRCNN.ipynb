{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, TimeDistributed, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "#create a training dataset for the model\n",
    "\n",
    "\n",
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \"\"\"Create a convolution layer to predict object scores and bounding boxes.\"\"\"\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "    return [x_class, x_regr, base_layers]\n",
    "\n",
    "def classifier_layer(base_layers, input_rois, num_rois, num_classes):\n",
    "    \"\"\"Define the classifier layer to classify object types and bounding boxes\"\"\"\n",
    "    pooling_regions = 7\n",
    "    x = TimeDistributed(tf.keras.layers.MaxPooling2D(pool_size=(7, 7)), input_shape=(num_rois,7,7,512))(input_rois)\n",
    "\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    x = TimeDistributed(Dense(4096, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dropout(0.5))(x)\n",
    "    x = TimeDistributed(Dense(4096, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dropout(0.5))(x)\n",
    "    out_class = TimeDistributed(Dense(num_classes, activation='softmax', kernel_initializer='zero'), name='dense_class')(x)\n",
    "    out_regr = TimeDistributed(Dense(num_classes * 4, activation='linear', kernel_initializer='zero'), name='dense_regress')(x)\n",
    "    return [out_class, out_regr]\n",
    "\n",
    "def get_faster_rcnn_model():\n",
    "    num_anchors = 9\n",
    "    num_rois = 32\n",
    "    num_classes = 4  # Adjust based on your dataset (including background)\n",
    "    pooling_regions = 7\n",
    "    # Base network (VGG16)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False)\n",
    "    base_layers = base_model.get_layer('block5_conv3').output\n",
    "\n",
    "    # Define inputs\n",
    "    input_shape = base_model.input_shape[1:]\n",
    "    img_input = Input(shape=input_shape)\n",
    "    roi_input = Input(shape=(num_rois, pooling_regions, pooling_regions, 512))\n",
    "\n",
    "    # Create RPN\n",
    "    rpn_classes, rpn_regressors, shared_layers = rpn_layer(base_layers, num_anchors)\n",
    "\n",
    "    # Create classifier\n",
    "    classifier_classes, classifier_regressors = classifier_layer(shared_layers, roi_input, num_rois, num_classes)\n",
    "\n",
    "    # Creating models\n",
    "    model_rpn = Model(inputs=img_input, outputs=[rpn_classes, rpn_regressors])\n",
    "    model_classifier = Model(inputs=[img_input, roi_input], outputs=[classifier_classes, classifier_regressors])\n",
    "\n",
    "    # Compile models\n",
    "    model_rpn.compile(optimizer='adam', loss=['binary_crossentropy', 'mean_squared_error'], metrics=['accuracy'])\n",
    "    model_classifier.compile(optimizer='adam', loss=['categorical_crossentropy', 'mean_squared_error'], metrics=['accuracy'])\n",
    "\n",
    "    return model_rpn, model_classifier\n",
    "\n",
    "\n",
    "\n",
    "def fit_and_evaluate(t_x, val_x, t_y, val_y, t_box, val_box, EPOCHS, BATCH_SIZE, class_w):\n",
    "    # Assuming `get_faster_rcnn_model()` is a function that builds and compiles the Faster R-CNN model\n",
    "    model_rpn, model_classifier = get_faster_rcnn_model()\n",
    "    \n",
    "    # Callbacks for training\n",
    "    erlstp = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint('faster_rcnn_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "    # Training the RPN\n",
    "    print(\"Training RPN...\")\n",
    "    model_rpn.fit(t_x, [t_y, t_box], batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                  validation_data=(val_x, [val_y, val_box]), \n",
    "                  class_weight=class_w, callbacks=[erlstp, reduce_lr, model_checkpoint], verbose=1)\n",
    "\n",
    "    # Training the Classifier\n",
    "    print(\"Training Classifier...\")\n",
    "    model_classifier.fit(t_x, [t_y, t_box], batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                         validation_data=(val_x, [val_y, val_box]), \n",
    "                         class_weight=class_w, callbacks=[erlstp, reduce_lr, model_checkpoint], verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"\\nValidation Score: \", model_classifier.evaluate(val_x, [val_y, val_box]))\n",
    "    return model_rpn.history, model_classifier.history\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_folds = 5\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "weight = {'class_weight': 1}  # Define as needed based on your class imbalances\n",
    "\n",
    "model_history = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \", i + 1)\n",
    "\n",
    "    # Assuming your data and labels are split into features (X), labels for classification (Y), and bounding boxes (B)\n",
    "    X_train, X_val, Y_train, Y_val, B_train, B_val = train_test_split(\n",
    "        X, Y, B, test_size=0.2, random_state=i)  # Ensure you have a proper split for X, Y, and B\n",
    "\n",
    "    X_train, Y_train, B_train = shuffle(X_train, Y_train, B_train, random_state=i)\n",
    "\n",
    "    history_rpn, history_classifier = fit_and_evaluate(X_train, X_val, Y_train, Y_val, B_train, B_val, epochs, batch_size, weight)\n",
    "    model_history.append((history_rpn, history_classifier))\n",
    "    print(\"=======\" * 12, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m\n\u001b[0;32m    113\u001b[0m detected_boxes \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod_1\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m    115\u001b[0m         [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.9\u001b[39m],  \u001b[38;5;66;03m# Example detection from method 1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# ... detections from other methods\u001b[39;00m\n\u001b[0;32m    122\u001b[0m }\n\u001b[0;32m    124\u001b[0m detection_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]  \u001b[38;5;66;03m# List of detection method names\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m evaluation_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_object_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mground_truth_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetected_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_methods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Print evaluation results for each method\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, metrics \u001b[38;5;129;01min\u001b[39;00m evaluation_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m, in \u001b[0;36mevaluate_object_detection\u001b[1;34m(ground_truth_boxes, detected_boxes, detection_methods)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt_boxes, det_boxes_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ground_truth_boxes, detected_boxes):\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;66;03m# Loop through each detection method\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;66;03m# for method, detections in det_boxes_dict.items():\u001b[39;00m\n\u001b[0;32m     28\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m detection_methods:\n\u001b[1;32m---> 29\u001b[0m     detections \u001b[38;5;241m=\u001b[39m \u001b[43mdet_boxes_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Calculate true positives, false positives, and false negatives\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     true_positives \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def evaluate_object_detection(ground_truth_boxes, detected_boxes, detection_methods):\n",
    "  \"\"\"\n",
    "  Evaluates object detection performance for multiple methods.\n",
    "\n",
    "  Args:\n",
    "      ground_truth_boxes: List of ground truth bounding boxes for each image.\n",
    "                          Each box is a list of [y_min, x_min, y_max, x_max] coordinates.\n",
    "      detected_boxes: List of detected bounding boxes for each image from different methods.\n",
    "                       Each element is a dictionary with keys as detection methods (strings)\n",
    "                       and values as lists of detections. Each detection is a list of\n",
    "                       [y_min, x_min, y_max, x_max, confidence_score] coordinates.\n",
    "      detection_methods: List of detection method names (strings).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary containing precision, recall, and F1-score for each detection method.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize empty dictionaries to store evaluation metrics\n",
    "  precisions = {method: [] for method in detection_methods}\n",
    "  recalls = {method: [] for method in detection_methods}\n",
    "  f1_scores = {method: [] for method in detection_methods}\n",
    "\n",
    "  # Loop through each image\n",
    "  for gt_boxes, det_boxes_dict in zip(ground_truth_boxes, detected_boxes):\n",
    "    # Loop through each detection method\n",
    "    # for method, detections in det_boxes_dict.items():\n",
    "    for method in detection_methods:\n",
    "      detections = det_boxes_dict[method]\n",
    "      # Calculate true positives, false positives, and false negatives\n",
    "      true_positives = 0\n",
    "      false_positives = 0\n",
    "      false_negatives = len(gt_boxes)\n",
    "\n",
    "      # Loop through each ground truth box\n",
    "      for gt_box in gt_boxes:\n",
    "        # Check for overlaps with detected boxes for this method\n",
    "        iou_max = 0\n",
    "        for det_box in detections:\n",
    "          iou = calculate_iou(gt_box, det_box)  # Replace with your IOU calculation function\n",
    "          iou_max = max(iou_max, iou)\n",
    "\n",
    "        # Consider a ground truth box detected if IoU is above a threshold (e.g., 0.5)\n",
    "        if iou_max >= 0.5:\n",
    "          true_positives += 1\n",
    "          false_negatives -= 1\n",
    "\n",
    "      # Calculate false positives from detections without matching ground truth boxes\n",
    "      false_positives = len(detections) - true_positives\n",
    "\n",
    "      # Calculate precision, recall, and F1-score\n",
    "      if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "      else:\n",
    "        precision = 0.0\n",
    "      if len(gt_boxes) > 0:\n",
    "        recall = true_positives / len(gt_boxes)\n",
    "      else:\n",
    "        recall = 0.0\n",
    "      if precision + recall > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "      else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "      # Append metrics for this method and image\n",
    "      precisions[method].append(precision)\n",
    "      recalls[method].append(recall)\n",
    "      f1_scores[method].append(f1_score)\n",
    "\n",
    "  # Calculate average precision, recall, and F1-score for each method\n",
    "  average_metrics = {}\n",
    "  for method in detection_methods:\n",
    "    average_metrics[method] = {\n",
    "        \"precision\": tf.reduce_mean(precisions[method]),\n",
    "        \"recall\": tf.reduce_mean(recalls[method]),\n",
    "        \"f1_score\": tf.reduce_mean(f1_scores[method]),\n",
    "    }\n",
    "\n",
    "  return average_metrics\n",
    "\n",
    "def calculate_iou(gt_box, det_box):\n",
    "  \"\"\"\n",
    "  Calculates Intersection-over-Union (IoU) between two bounding boxes.\n",
    "\n",
    "  Args:\n",
    "      gt_box: Ground truth bounding box (list of [y_min, x_min, y_max, x_max] coordinates).\n",
    "      det_box: Detected bounding box (list of [y_min, x_min, y_max, x_max, confidence_score] coordinates).\n",
    "\n",
    "  Returns:\n",
    "      IoU value (float) between 0 and 1.\n",
    "  \"\"\"\n",
    "  # Calculate area of overlap and area of union\n",
    "  ymin_intersection = max(gt_box[0], det_box[0])\n",
    "  xmin_intersection = max(gt_box[1], det_box[1])\n",
    "  ymax_intersection = min(gt_box[2], det_box[2])\n",
    "  xmax_intersection = min(gt_box[3], det_box[3])\n",
    "\n",
    "  area_intersection = max(0, xmax_intersection - xmin_intersection) * max(0, ymax_intersection - ymin_intersection)\n",
    "  area_gt = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "  area_det = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])\n",
    "  area_union = area_gt + area_det - area_intersection\n",
    "\n",
    "  # Calculate IoU\n",
    "  iou = area_intersection / (area_union + 1e-10)  # Add a small value to avoid division by zero\n",
    "  return iou\n",
    "\n",
    "# Example usage\n",
    "ground_truth_boxes = [\n",
    "    [0.2, 0.3, 0.7, 0.8],  # Example ground truth bounding box\n",
    "    # ... more ground truth boxes\n",
    "]\n",
    "\n",
    "detected_boxes = {\n",
    "    \"method_1\": [\n",
    "        [0.1, 0.2, 0.8, 0.9, 0.9],  # Example detection from method 1\n",
    "        # ... more detections for method 1\n",
    "    ],\n",
    "    \"method_2\": [\n",
    "        # ... detections from method 2\n",
    "    ],\n",
    "    # ... detections from other methods\n",
    "}\n",
    "\n",
    "detection_methods = [\"method_1\", \"method_2\", ...]  # List of detection method names\n",
    "\n",
    "evaluation_results = evaluate_object_detection(ground_truth_boxes, detected_boxes, detection_methods)\n",
    "\n",
    "# Print evaluation results for each method\n",
    "for method, metrics in evaluation_results.items():\n",
    "  print(f\"Method: {method}\")\n",
    "  print(f\"  Precision: {metrics['precision']}\")\n",
    "  print(f\"  Recall: {metrics['recall']}\")\n",
    "  print(f\"  F1-Score: {metrics['f1_score']}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3333333333333333\n",
      "Recall: 0.5\n",
      "F1 Score: 0.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_detection(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate object detection performance using precision, recall, and F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    - gt_boxes: List of ground truth bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - pred_boxes: List of predicted bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - iou_threshold: IoU threshold for considering a detection as a true positive.\n",
    "\n",
    "    Returns:\n",
    "    - precision: Precision of the detection.\n",
    "    - recall: Recall of the detection.\n",
    "    - f1_score: F1 score of the detection.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(gt_boxes) == 0 or len(pred_boxes) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "    pred_boxes = np.array(pred_boxes)\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for pred_box in pred_boxes:\n",
    "        iou = calculate_iou(gt_boxes, pred_box)\n",
    "        if np.max(iou) >= iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    false_negatives = len(gt_boxes) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_iou(gt_boxes, pred_box):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between a predicted bounding box and a list of ground truth bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    - gt_boxes: List of ground truth bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - pred_box: Predicted bounding box in format [x_min, y_min, x_max, y_max].\n",
    "\n",
    "    Returns:\n",
    "    - iou: IoU value between the predicted box and each ground truth box.\n",
    "    \"\"\"\n",
    "    x1 = np.maximum(gt_boxes[:, 0], pred_box[0])\n",
    "    y1 = np.maximum(gt_boxes[:, 1], pred_box[1])\n",
    "    x2 = np.minimum(gt_boxes[:, 2], pred_box[2])\n",
    "    y2 = np.minimum(gt_boxes[:, 3], pred_box[3])\n",
    "\n",
    "    intersection_area = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
    "    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "    pred_box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "\n",
    "    iou = intersection_area / (gt_box_area + pred_box_area - intersection_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Example usage:\n",
    "gt_boxes = [[100, 100, 200, 200], [300, 300, 400, 400]]\n",
    "pred_boxes = [[90, 90, 210, 210], [320, 320, 420, 420], [500, 500, 600, 600]]\n",
    "iou_threshold = 0.5\n",
    "\n",
    "precision, recall, f1_score = evaluate_detection(gt_boxes, pred_boxes, iou_threshold)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            YOLOv4       HOG\n",
      "Precision                 0.333333  0.666667\n",
      "Recall                         0.5       1.0\n",
      "F1 Score                       0.4       0.8\n",
      "Human Detection Accuracy  100.00 %   80.00 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculation_accuracy(detection_per_frame, ground_truth_per_frame):\n",
    "   \"\"\"\n",
    "   param: detection_per_frame - list of integers representing number of human detections per frame.\n",
    "   param: ground_truth_per_frame - list of integers representing number of actual humans in the frame.\n",
    "   \"\"\"\n",
    "   if len(detection_per_frame) != len(ground_truth_per_frame):\n",
    "      raise ValueError(\"Different number of detections and ground truths\")\n",
    "   \n",
    "   correct_detections = sum(1 for detected, actual in zip(detection_per_frame, ground_truth_per_frame) if detected == actual)\n",
    "   accuracy = correct_detections / len(ground_truth_per_frame) \n",
    "   return f'{accuracy*100:.2f} %'\n",
    "\n",
    "def evaluate_detection(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate object detection performance using precision, recall, and F1 score.\n",
    "\n",
    "    Parameters:\n",
    "    - gt_boxes: List of ground truth bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - pred_boxes: List of predicted bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - iou_threshold: IoU threshold for considering a detection as a true positive.\n",
    "\n",
    "    Returns:\n",
    "    - precision: Precision of the detection.\n",
    "    - recall: Recall of the detection.\n",
    "    - f1_score: F1 score of the detection.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(gt_boxes) == 0 or len(pred_boxes) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "    pred_boxes = np.array(pred_boxes)\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for pred_box in pred_boxes:\n",
    "        iou = calculate_iou(gt_boxes, pred_box)\n",
    "        if np.max(iou) >= iou_threshold:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "\n",
    "    false_negatives = len(gt_boxes) - true_positives\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision == 0 or recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_iou(gt_boxes, pred_box):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) between a predicted bounding box and a list of ground truth bounding boxes.\n",
    "\n",
    "    Parameters:\n",
    "    - gt_boxes: List of ground truth bounding boxes in format [x_min, y_min, x_max, y_max].\n",
    "    - pred_box: Predicted bounding box in format [x_min, y_min, x_max, y_max].\n",
    "\n",
    "    Returns:\n",
    "    - iou: IoU value between the predicted box and each ground truth box.\n",
    "    \"\"\"\n",
    "    x1 = np.maximum(gt_boxes[:, 0], pred_box[0])\n",
    "    y1 = np.maximum(gt_boxes[:, 1], pred_box[1])\n",
    "    x2 = np.minimum(gt_boxes[:, 2], pred_box[2])\n",
    "    y2 = np.minimum(gt_boxes[:, 3], pred_box[3])\n",
    "\n",
    "    intersection_area = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n",
    "    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "    pred_box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
    "    area_union = gt_box_area + pred_box_area - intersection_area\n",
    "    iou = intersection_area / area_union \n",
    "\n",
    "    return iou\n",
    "\n",
    "def print_evaluation_table(detection_methods, results, results2):\n",
    "    \"\"\"\n",
    "    Print a table with precision, recall, and F1 scores for multiple detection methods.\n",
    "\n",
    "    Parameters:\n",
    "    - detection_methods: List of names of detection methods.\n",
    "    - results: Dictionary containing evaluation results for each method.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results, index=['Precision', 'Recall', 'F1 Score'])\n",
    "    df.columns.name = 'Detection Method'\n",
    "    # Convert accuracy results dictionary to DataFrame\n",
    "    accuracy_df = pd.DataFrame(results2, index=['Human Detection Accuracy'])\n",
    "    \n",
    "    # Merge accuracy DataFrame with main DataFrame\n",
    "    df = pd.concat([df, accuracy_df])\n",
    "    # df['Accuracy'] = pd.Series(results2)\n",
    "    \n",
    "    print(df)\n",
    "\n",
    "# Example usage:\n",
    "gt_boxes = [[100, 100, 200, 200], [300, 300, 400, 400]]\n",
    "pred_boxes_method1 = [[90, 90, 210, 210], [320, 320, 420, 420], [500, 500, 600, 600]]\n",
    "pred_boxes_method2 = [[100, 100, 200, 200], [310, 310, 410, 410], [520, 520, 620, 620]]\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Example usage (assuming you have these lists from your video processing)\n",
    "yolo_detections = [3, 2, 4, 3, 5]  # Example detection counts per frame from YOLO\n",
    "hog_detections = [2, 2, 4, 3, 5]   # Example detection counts per frame from HOG\n",
    "ground_truth = [3, 2, 4, 3, 5]     # Actual counts of humans per frame\n",
    "\n",
    "results = {}\n",
    "results['YOLOv4'] = evaluate_detection(gt_boxes, pred_boxes_method1, iou_threshold)\n",
    "results['HOG'] = evaluate_detection(gt_boxes, pred_boxes_method2, iou_threshold)\n",
    "# print_evaluation_table(['YOLOv4', 'HOG'], results)\n",
    "\n",
    "results2={}\n",
    "results2['YOLOv4'] = calculation_accuracy(yolo_detections, ground_truth)\n",
    "results2['HOG'] = calculation_accuracy(hog_detections, ground_truth)\n",
    "print_evaluation_table(['YOLOv4', 'HOG'], results, results2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "def detect_objects(video_path, weights_path, config_path, class_names_path): \n",
    "    # Initialize the network \n",
    "    net = cv2.dnn_DetectionModel(weights_path, config_path) \n",
    "    net.setInputSize(416, 416) \n",
    "    net.setInputScale(1.0 / 255) \n",
    "    net.setInputSwapRB(True)\n",
    "    # Load class labels\n",
    "    try:\n",
    "        with open(class_names_path, \"r\") as f:\n",
    "            classes = [line.strip() for line in f.readlines()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Class names file not found.\")\n",
    "        return\n",
    "\n",
    "    # Initialize counters and lists for detections\n",
    "    total_human_detections = 0\n",
    "    total_non_human_detections = 0\n",
    "    human_detections_per_frame = []\n",
    "    non_human_detections_per_frame = []\n",
    "\n",
    "    # Lists to store predicted bounding boxes\n",
    "    all_pred_boxes = []\n",
    "\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform detection\n",
    "        class_ids, confidences, boxes = net.detect(frame, confThreshold=0.5, nmsThreshold=0.4)\n",
    "\n",
    "        # Store predicted bounding boxes\n",
    "        all_pred_boxes.append(boxes)\n",
    "        \n",
    "        # Initialize counters for detections in the current frame\n",
    "        frame_human_detections = 0\n",
    "        frame_non_human_detections = 0\n",
    "        \n",
    "        # Count detections and draw bounding boxes\n",
    "        for (class_id, confidence, box) in zip(class_ids, confidences, boxes):\n",
    "            class_name = classes[int(class_id)]\n",
    "            if class_name == 'person':\n",
    "                # If the detected class is 'person', draw a green bounding box\n",
    "                color = (0, 255, 0)  # Green color for humans\n",
    "                frame_human_detections += 1\n",
    "            else:\n",
    "                # For non-human objects, use a blue bounding box\n",
    "                color = (255, 0, 0)  # Blue color for non-human objects\n",
    "                frame_non_human_detections += 1\n",
    "            \n",
    "            # Draw bounding boxes around detected objects\n",
    "            x, y, w, h = box\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Update total and per-frame detections\n",
    "        total_human_detections += frame_human_detections\n",
    "        total_non_human_detections += frame_non_human_detections\n",
    "        human_detections_per_frame.append(frame_human_detections)\n",
    "        non_human_detections_per_frame.append(frame_non_human_detections)\n",
    "        \n",
    "        # Optionally, print the number of detections in the current frame\n",
    "        print(f\"Humans detected in this frame: {frame_human_detections}, Non-human objects detected in this frame: {frame_non_human_detections}\")\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Calculate and print the mean of human and non-human detections\n",
    "    mean_human_detections = np.mean(human_detections_per_frame)\n",
    "    mean_non_human_detections = np.mean(non_human_detections_per_frame)\n",
    "    print(\"Mean number of humans detected per frame:\", mean_human_detections)\n",
    "    print(\"Mean number of non-human objects detected per frame:\", mean_non_human_detections)\n",
    "\n",
    "    # Print the detection summary for the entire video\n",
    "    print(\"Total humans detected in the video:\", total_human_detections)\n",
    "    print(\"Total non-human objects detected in the video:\", total_non_human_detections)\n",
    "    # Display the mean as the \"correct\" number of detections for demonstration\n",
    "    print(\"Correct number of humans detected (based on mean):\", round(mean_human_detections))\n",
    "    print(\"Correct number of non-human objects detected (based on mean):\", round(mean_non_human_detections))\n",
    "\n",
    "    return all_pred_boxes\n",
    "\n",
    "video_path = \"C:\\\\Users\\\\TheEarthG\\\\Downloads\\\\human.mp4\" \n",
    "weights_path = \"C:\\\\Users\\\\TheEarthG\\\\Downloads\\\\yolov4.weights\" \n",
    "config_path = \"C:\\\\Users\\\\TheEarthG\\\\Downloads\\\\yolov4.cfg\" \n",
    "class_names_path = \"C:\\\\Users\\\\TheEarthG\\\\Downloads\\\\coco.names\"\n",
    "\n",
    "all_pred_boxes = detect_objects(video_path, weights_path, config_path, class_names_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m]     \u001b[38;5;66;03m# Actual counts of humans per frame\u001b[39;00m\n\u001b[0;32m    116\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 117\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOLOv4\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_boxes_method1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHOG\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m evaluate_model(gt_boxes, pred_boxes_method2, iou_threshold)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# print_evaluation_table(['YOLOv4', 'HOG'], results)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[44], line 61\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(gt_bbox, pred_bbox, iou_threshold)\u001b[0m\n\u001b[0;32m     58\u001b[0m false_negatives \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred_box \u001b[38;5;129;01min\u001b[39;00m pred_boxes:\n\u001b[1;32m---> 61\u001b[0m   iou \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_box\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmax(iou) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m iou_threshold:\n\u001b[0;32m     63\u001b[0m       true_positives \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[44], line 17\u001b[0m, in \u001b[0;36mcalculate_iou\u001b[1;34m(gt_box, det_box)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mCalculates Intersection-over-Union (IoU) between two bounding boxes.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    IoU of 0.5 is considered as threshold to classify detection as true positive.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate area of overlap and area of union\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m ymin_intersection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgt_box\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdet_box\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m xmin_intersection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(gt_box[\u001b[38;5;241m1\u001b[39m], det_box[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m ymax_intersection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(gt_box[\u001b[38;5;241m2\u001b[39m], det_box[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_iou(gt_box, det_box):\n",
    "  \"\"\"\n",
    "  Calculates Intersection-over-Union (IoU) between two bounding boxes.\n",
    "\n",
    "  Args:\n",
    "      gt_box: Ground truth bounding box (list of [y_min, x_min, y_max, x_max] coordinates).\n",
    "      det_box: Detected bounding box (list of [y_min, x_min, y_max, x_max] coordinates).\n",
    "\n",
    "  Returns:\n",
    "      IoU value (float) between 0 and 1. \n",
    "      IoU of 0.5 is considered as threshold to classify detection as true positive.\n",
    "  \"\"\"\n",
    "  # Calculate area of overlap and area of union\n",
    "  ymin_intersection = max(gt_box[0], det_box[0])\n",
    "  xmin_intersection = max(gt_box[1], det_box[1])\n",
    "  ymax_intersection = min(gt_box[2], det_box[2])\n",
    "  xmax_intersection = min(gt_box[3], det_box[3])\n",
    "\n",
    "  area_intersection = max(0, xmax_intersection - xmin_intersection) * max(0, ymax_intersection - ymin_intersection)\n",
    "  area_gt = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
    "  area_det = (det_box[2] - det_box[0]) * (det_box[3] - det_box[1])\n",
    "  area_union = area_gt + area_det - area_intersection\n",
    "\n",
    "  # Calculate IoU\n",
    "  iou = area_intersection / area_union if area_union != 0 else 0 # Add a small value to avoid division by zero\n",
    "  return iou\n",
    "\n",
    "def evaluate_model(gt_bbox, pred_bbox, iou_threshold=0.5):\n",
    "  \n",
    "  \"\"\"\n",
    "  Evaluates a model's performance using Intersection over Union (IoU).\n",
    "  \n",
    "  Args:\n",
    "    gt_bbox: A list of lists containing the ground truth bounding boxes for each image in the dataset. \n",
    "             Each bounding box should be a list of four integers representing [ymin, xmin, ymax, xmax].\n",
    "    pred_bbox: A list of predicted bounding boxes in the same format as `gt_bbox`.\n",
    "    \n",
    "  Kwargs:\n",
    "    iou_threshold: The minimum IoU required for a prediction to count as correct. Default is 0.5.\n",
    "\n",
    "  Returns:\n",
    "    - precision: Precision of the detection.\n",
    "    - recall: Recall of the detection.\n",
    "    - f1_score: F1 score of the detection.\n",
    "  \"\"\"\n",
    "  #if the list of ground truth bounding box or prediction bounding box is empty then we can't calculate the precision, recall and f1.\n",
    "  if len(gt_bbox) == 0 or len(pred_bbox) == 0:\n",
    "        return 0, 0, 0\n",
    "  \n",
    "  gt_bbox = np.array(gt_bbox)\n",
    "  pred_boxes = np.array(pred_bbox)\n",
    "\n",
    "  true_positives = 0\n",
    "  false_positives = 0\n",
    "  false_negatives = 0\n",
    "\n",
    "  for pred_box in pred_boxes:\n",
    "    iou = calculate_iou(gt_bbox, pred_box)\n",
    "    if np.max(iou) >= iou_threshold:\n",
    "        true_positives += 1\n",
    "    else:\n",
    "        false_positives += 1\n",
    "\n",
    "  false_negatives = len(gt_bbox) - true_positives\n",
    "\n",
    "  precision = true_positives / (true_positives + false_positives)\n",
    "  recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "  if precision == 0 or recall == 0:\n",
    "    f1_score = 0\n",
    "  else:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "  return precision, recall, f1_score\n",
    "\n",
    "def calculation_accuracy(detection_per_frame, ground_truth_per_frame):\n",
    "   \"\"\"\n",
    "   param: detection_per_frame - list of integers representing number of human detections per frame.\n",
    "   param: ground_truth_per_frame - list of integers representing number of actual humans in the frame.\n",
    "   \"\"\"\n",
    "   if len(detection_per_frame) != len(ground_truth_per_frame):\n",
    "      raise ValueError(\"Different number of detections and ground truths\")\n",
    "   \n",
    "   correct_detections = sum(1 for detected, actual in zip(detection_per_frame, ground_truth_per_frame) if detected == actual)\n",
    "   accuracy = correct_detections / len(ground_truth_per_frame) \n",
    "   return f'{accuracy*100:.2f}%'\n",
    "\n",
    "def print_evaluation_table(detection_methods, results, results2):\n",
    "    \"\"\"\n",
    "    Print a table with precision, recall, and F1 scores for multiple detection methods.\n",
    "\n",
    "    Parameters:\n",
    "    - detection_methods: List of names of detection methods.\n",
    "    - results: Dictionary containing evaluation results for each method.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results, index=['Precision', 'Recall', 'F1 Score'])\n",
    "    df.columns.name = 'Detection Method'\n",
    "    accuracy_df = pd.DataFrame(results2, index=['Human Detection Accuracy'])\n",
    "    df = pd.concat([df, accuracy_df])\n",
    "    print(df)\n",
    "\n",
    "#Example usage. It supposes that you have list of bounding boxes coordinates both predicted and ground truth one.\n",
    "gt_boxes = [[100, 100, 200, 200], [300, 300, 400, 400]]\n",
    "pred_boxes_method1 = [[90, 90, 210, 210], [320, 320, 420, 420], [500, 500, 600, 600]]\n",
    "pred_boxes_method2 = [[100, 100, 200, 200], [310, 310, 410, 410], [520, 520, 620, 620]]\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Example usage (assuming you have these lists from your video processing)\n",
    "yolo_detections = [3, 2, 4, 3, 5]  # Example detection counts per frame from YOLO\n",
    "hog_detections = [2, 2, 4, 3, 5]   # Example detection counts per frame from HOG\n",
    "ground_truth = [3, 2, 4, 3, 5]     # Actual counts of humans per frame\n",
    "\n",
    "results = {}\n",
    "results['YOLOv4'] = evaluate_model(gt_boxes, pred_boxes_method1, iou_threshold)\n",
    "results['HOG'] = evaluate_model(gt_boxes, pred_boxes_method2, iou_threshold)\n",
    "# print_evaluation_table(['YOLOv4', 'HOG'], results)\n",
    "\n",
    "results2={}\n",
    "results2['YOLOv4'] = calculation_accuracy(yolo_detections, ground_truth)\n",
    "results2['HOG'] = calculation_accuracy(hog_detections, ground_truth)\n",
    "print_evaluation_table(['YOLOv4', 'HOG'], results, results2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OutlierWork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
