{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def backward(self, target: np.ndarray, out: np.ndarray) -> float:\n",
    "  \"\"\"One backward pass through all the layers of the neural network.\n",
    "  During this phase we calculate the gradients of the loss with respect to\n",
    "  each of the parameters of the entire neural network. Most of the heavy\n",
    "  lifting is done by the `backward` methods of the layers, so this method\n",
    "  should be relatively simple. Also make sure to compute the loss in this\n",
    "  method and NOT in `self.forward`.\n",
    "\n",
    "  Note: Both input arrays have the same shape.\n",
    "\n",
    "  Parameters:\n",
    "      target (np.ndarray): The targets we are trying to fit to (e.g., training labels).\n",
    "      out (np.ndarray): The predictions of the model on training data.\n",
    "\n",
    "  Returns:\n",
    "      float: The loss of the model given the training inputs and targets.\n",
    "  \"\"\"\n",
    "\n",
    "  # Compute the loss based on your chosen loss function (replace with your specific loss function)\n",
    "  loss = np.mean(np.square(target - out))  # Example: Mean squared error\n",
    "\n",
    "  # Backpropagate through the network's layers, starting from the output layer\n",
    "  dL_dy = 2.0 * (out - target)  # Derivative of the loss function (MSE) w.r.t. output layer activations\n",
    "\n",
    "  # Iterate backwards through layers (assuming your network has `self.num_layers` layers)\n",
    "  for layer in reversed(range(1, self.num_layers)):  # Skip the input layer (layer 0)\n",
    "    W_l = self.weights[layer]  # Weights of the current layer\n",
    "    b_l = self.biases[layer]  # Biases of the current layer\n",
    "    a_prev = self.activations[layer - 1]  # Activations from the previous layer\n",
    "\n",
    "    # Calculate the derivative of the activation function (replace with your specific activation function)\n",
    "    da_l = self.activation_functions[layer - 1].derivative(a_prev)\n",
    "\n",
    "    # Backpropagate the gradients using the chain rule\n",
    "    dL_da_l = dL_dy * da_l  # Gradient w.r.t. current layer activations\n",
    "\n",
    "    # Update gradients for weights and biases of the current layer\n",
    "    self.dW[layer] = np.mean(dL_da_l[:, np.newaxis] * a_prev, axis=0)  # Update weights\n",
    "    self.db[layer] = np.mean(dL_da_l, axis=0)  # Update biases\n",
    "\n",
    "    # Update dL_dy for the next layer (chain rule)\n",
    "    dL_dy = np.dot(dL_da_l, W_l.T)  # Backpropagate gradient to previous layer\n",
    "\n",
    "  return loss  # Return the calculated loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, target: np.ndarray, out: np.ndarray, loss_fn) -> float:\n",
    "    \"\"\"One backward pass through all the layers of the neural network.\n",
    "    During this phase we calculate the gradients of the loss with respect to\n",
    "    each of the parameters of the entire neural network. Most of the heavy\n",
    "    lifting is done by the `backward` methods of the layers, so this method\n",
    "    should be relatively simple. Also make sure to compute the loss in this\n",
    "    method and NOT in `self.forward`.\n",
    "\n",
    "    Note: Both input arrays have the same shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target  the targets we are trying to fit to (e.g., training labels)\n",
    "    out     the predictions of the model on training data\n",
    "    loss_fn the loss function to compute the loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the loss of the model given the training inputs and targets\n",
    "    \"\"\"\n",
    "    # Compute the loss.\n",
    "    loss = loss_fn(out, target)\n",
    "    \n",
    "    # Initialize gradient of loss with respect to the output.\n",
    "    grad_loss_output = loss_fn.backward(out, target)\n",
    "    \n",
    "    # Backpropagate through the network's layers.\n",
    "    for layer in reversed(self.layers):\n",
    "        grad_loss_output = layer.backward(grad_loss_output)\n",
    "\n",
    "    return loss\n",
    "loss_function = initialize_loss('mse')  # Example: using Mean Squared Error loss\n",
    "# Assuming `target` and `out` are already defined\n",
    "network.backward(target, out, loss_function)\n",
    "\n",
    "def mse_loss(output, target):\n",
    "    return np.mean((output - target) ** 2)\n",
    "\n",
    "def mse_loss_backward(output, target):\n",
    "    return 2 * (output - target) / len(output)\n",
    "network.backward(target, out, mse_loss)\n",
    "\n",
    "gradient = (out-target)/ out.shape[0]\n",
    "    for layer in reversed(self.layers):\n",
    "\t    gradient = layer.backward(gradient)\n",
    "    for layer in self.layers:\n",
    "\t    self.optimizer.update(layer.parameters, layer.gradients)\n",
    "\t#compute the loss\n",
    "    loss = self.loss\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, target: np.ndarray, out: np.ndarray, loss_fn) -> float:\n",
    "    \"\"\"One backward pass through all the layers of the neural network.\n",
    "    During this phase we calculate the gradients of the loss with respect to\n",
    "    each of the parameters of the entire neural network. Most of the heavy\n",
    "    lifting is done by the `backward` methods of the layers, so this method\n",
    "    should be relatively simple. Also make sure to compute the loss in this\n",
    "    method and NOT in `self.forward`.\n",
    "\n",
    "    Note: Both input arrays have the same shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target  the targets we are trying to fit to (e.g., training labels)\n",
    "    out     the predictions of the model on training data\n",
    "    loss_fn the loss function to compute the loss\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    the loss of the model given the training inputs and targets\n",
    "    \"\"\"\n",
    "    gradient = (out - target)/ out.shape[0]\n",
    "    for layer in reversed(self.layers):\n",
    "        gradient = layer.backward(gradient)\n",
    "    for layer in self.layers:\n",
    "        self.optimizer.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    #Compute loss - we assume that the self.loss initialized in the __init__ method will handle loss.\n",
    "    loss = self.loss\n",
    "    return loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"One forward pass through all the layers of the neural network.\n",
    "    Param: X  design matrix whose must match the input shape required by the first layer\n",
    "    Returns: forward pass output, matches the shape of the output of the last layer\n",
    "    \"\"\"\n",
    "    # Make a copy of the input data.\n",
    "    input = X.copy()\n",
    "\n",
    "    # Iterate through the network's layers.\n",
    "    for layer in self.layers:\n",
    "        # Compute the output of the current layer.\n",
    "        input = layer.forward(input)\n",
    "\n",
    "    # Return the output of the last layer.\n",
    "    return input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "  \"\"\"One forward pass through all the layers of the neural network.\n",
    "  Param: X design matrix that must match the input shape required by the first layer.\n",
    "  Returns: forward pass output, matches the shape of the output of the last layer.\n",
    "  \"\"\"\n",
    "  # Make a copy of the input data.\n",
    "  input = X.copy()\n",
    "\n",
    "  # Iterate through the network's layers.\n",
    "  for layer in self.layers:\n",
    "    # Compute the output of the current layer.\n",
    "    input = layer.forward(input)\n",
    "\n",
    "    # Return the output of the last layer.\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"One forward pass through all the layers of the neural network.\n",
    "    Param: X  design matrix whose must match the input shape required by the first layer\n",
    "    Returns: forward pass output, matches the shape of the output of the last layer\n",
    "    \"\"\"\n",
    "    # Make a copy of the input data.\n",
    "    input = X.copy()\n",
    "\n",
    "    # Iterate through the network's layers.\n",
    "    for layer in self.layers:\n",
    "        # Compute the output of the current layer.\n",
    "        input = layer.forward(input)\n",
    "\n",
    "    # Return the output of the last layer.\n",
    "    return input\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OutlierWork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
